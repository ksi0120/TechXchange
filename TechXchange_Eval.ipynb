{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# RAG Metrics\n\nThe list below is some of the metrics that are used to measure the performance of a RAG model.\nAs the peformance depends on 3 difference components: Answer Synthesis (LLM Parameters and Prompt), Retreival (Search parameters and ranking), and Indexing (Embedding model and chunking methods), each of the metrics attempt at measuring different components. The list below is not exhaustive and there are different metrics for different use cases, for example, measuring the tone of the final answer for customer facing models.\n\n**Correctness**: \n\nWhether the generated answer matches that of the reference answer given the query (requires labels).\n\n**Semantic Similarity**: \n\nWhether the predicted answer is semantically similar to the reference answer (requires labels).\n\n**Faithfulness**: \n\nEvaluates if the answer is faithful to the retrieved contexts (in other words, whether if there's hallucination).\n\n**Context Relevancy**: \n\nWhether retrieved context is relevant to the query. \n\n**Answer Relevancy**: \n\nWhether the generated answer is relevant to the query. \n\n**Guideline Adherence**: \n\nWhether the predicted answer adheres to specific guidelines.\n\n**Hit Rate**:\n\nHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it\u2019s about how often our system gets it right within the top few guesses.\n\n**Mean Reciprocal Rank (MRR)**:\n\nMRR serves as a metric for evaluating the accuracy of a system by examining the rank of the highest-placed relevant document for each query. It calculates the average of the reciprocals of these ranks across all queries. For instance, if the first relevant document is ranked highest, the reciprocal rank is 1; if it\u2019s second, the reciprocal rank is 1/2, and so forth.", "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "id": "e6d9ecc4-f76b-4aea-abc8-3f9afdb20ed6"}}, {"cell_type": "markdown", "source": "# Preparation", "metadata": {"id": "28ada75e-2110-4fbb-a53a-516a7f518a2d"}}, {"cell_type": "markdown", "source": "Run the cell below and restart the kernel. Once the kernel restarts, you and skip to **Set Up**", "metadata": {"id": "420d046a-4b2d-465f-b31a-11d0539cf41e"}}, {"cell_type": "code", "source": "!pip install -q -U llama-index llama-index-llms-ibm pydantic\nprint(\"Done Installing!\")", "metadata": {"id": "8b85de4b-0df5-467f-9b34-69cf5482fe8f"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Set Up", "metadata": {"id": "1a70301a-26e4-46bf-b087-3d6df171e665"}}, {"cell_type": "markdown", "source": "Before we measure the performances, we need a way to tell WatsonX.AI that it's an authroized user trying to use the models. Follow the instruction below to retrieve your API Key.\n\nInstruction:\n\nWhen done, paste **your api key below** and run both cells", "metadata": {"id": "ba97c771-bda7-4161-84f4-bcb3994ebe33"}}, {"cell_type": "code", "source": "myApikey=\"Paste your API KEY Here\"", "metadata": {"id": "cfc1cf06-ca02-4f70-a1d0-8c38a9608cc7"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "\nfrom llama_index.core.evaluation import BatchEvalRunner\nfrom llama_index.core.evaluation import AnswerRelevancyEvaluator\nfrom llama_index.core.evaluation import CorrectnessEvaluator\nfrom llama_index.llms.ibm import WatsonxLLM\nfrom statistics import mean\nimport os\nimport nest_asyncio\nimport asyncio\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nnest_asyncio.apply()\n\nwatsonx_llm = WatsonxLLM(\n    model_id=\"mistralai/mistral-large\", # granite or mistral \n    url=\"https://us-south.ml.cloud.ibm.com\",\n    apikey=myApikey,\n    project_id=os.getenv(\"PROJECT_ID\"),\n    max_new_tokens=1000\n)", "metadata": {"id": "1df43797-7cf3-4648-8d59-f5101ed5c143"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "async def evaluate_rag(log,ground_truth=None):\n    # Ingest the session log and grount truth label to compute the RAG metrics\n    # The metrics to be computed are: Answer Relevancy, Correctness, and Semantic Similiarity\n    parsed_list=log.split(\"\\n\") # take the log from the session log and turn it into a list\n    filtered=[x for x in parsed_list if \"<|system|>\" not in x and \"<|assistant|>\" not in x and \"<|user|>\" not in x and len(x)>5]\n    queries=filtered[1::2] #extract the queries only\n    responses=filtered[2::2] #extract the responses only\n    \n    print(\"Your queries are: {0}{1}\".format('\\n\\n',' \\n'.join(queries)))\n    print(\"\\nYour responses are: {0}{1}\".format('\\n\\n',' \\n'.join(responses)))\n    \n    ar_eval=AnswerRelevancyEvaluator(llm=watsonx_llm)\n    cr_eval=CorrectnessEvaluator(llm=watsonx_llm)\n    #ss_eval=SemanticSimilarityEvaluator(llm=watsonx_llm)\n    \n    ar_runner = BatchEvalRunner(\n        {\"answer_relevancy\": ar_eval},workers=5,show_progress=True)\n\n    cr_ss_runner=BatchEvalRunner(\n        {\"correctness\": cr_eval},workers=5,show_progress=True)\n\n    eval_results_ar = await ar_runner.aevaluate_response_strs(queries=queries, response_strs=responses)\n    ar_score=mean([float(eval_results_ar['answer_relevancy'][x].score) for x in range(len(queries))])*100\n    print(f\"The average Answer Relevancy score is {round(ar_score,2)}%\")\n    \n    #eval_results_crss = await runner.aevaluate_response_strs(queries=queries, response_strs=responses,reference=ground_truth)\n\n    ar_score=mean([float(eval_results_ar['answer_relevancy'][x].score) for x in range(len(queries))])*100\n    print(\"\\n\")\n    \n    print(f\"The average Answer Relevancy score is {round(ar_score,2)}%\")\n    \n\n\n\n    \n\n    ", "metadata": {"id": "d8beea0d-6147-4628-a51d-9673edacd292"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "With the LLM connection establsihed, copy paste your RAG log from your saved Prompt Lab session", "metadata": {"id": "dad293c7-e252-4cc4-9509-5d8678ba18e0"}}, {"cell_type": "code", "source": "myLog=\"\"\"<|system|>\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Avoid repeating information unless asked.\n<|user|>\nWhat is mega yacht\n<|assistant|>\nA mega yacht is a type of large yacht that is typically over 80 feet in length overall. These vessels are often used for leisure, recreational, or educational purposes and are available for hire in the inland and offshore waters of Puerto Rico. To be considered a mega yacht, the vessel must meet certain criteria, such as being available in Puerto Rico for tourist activities for at least six months out of the year and submitting quarterly reports to the Tourism Office detailing its usage in the tourist activity.\n<|user|>\nWhat is an existing business\n<|assistant|>\nAn existing business, as referred to in the document, is a business that is already engaged in a Tourist Activity at the time of filing an application for incentives under Chapter 5 of Subtitle B of this Code or which otherwise does not qualify as a New Tourism Business. This means that the business has already started operating and is generating revenue in the Tourist Activity sector.\n<|user|>\nWhat are the qualifications\n<|assistant|>\nTo become a Qualified Promoter in Puerto Rico, one must hold a bachelor's degree from an accredited university and have at least five years of professional experience in their area of competence, which can include fields such as accounting, finance, marketing, planning, international trade, law, economics, science, engineering, real estate, or other similar fields.\n<|assistant|>\"\"\"\n", "metadata": {"id": "edf9eb1e-d575-4add-a11b-ba9001c19048"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "To evaluate the answer correctness, we've prepared the ground truth below", "metadata": {"id": "8f8db10a-0e05-49a2-a8d3-a6cbf427c07d"}}, {"cell_type": "code", "source": "await evaluate_rag(myLog)", "metadata": {"id": "778a4bd0-91d1-4fc0-a8c9-50710c8136d6"}, "outputs": [], "execution_count": null}]}
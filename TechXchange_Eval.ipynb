{"metadata": {"kernelspec": {"display_name": "Python 3.11", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# RAG Metrics\n\nThe list below is some of the metrics that are used to measure the performance of a RAG model.\nAs the peformance depends on 3 difference components: Answer Synthesis (LLM Parameters and Prompt), Retreival (Search parameters and ranking), and Indexing (Embedding model and chunking methods), each of the metrics attempt at measuring different components. The list below is not exhaustive and there are different metrics for different use cases, for example, measuring the tone of the final answer for customer facing models.\n\n**Correctness**: \n\nWhether the generated answer matches that of the reference answer given the query (requires labels).\n\n**Semantic Similarity**: \n\nWhether the predicted answer is semantically similar to the reference answer (requires labels).\n\n**Faithfulness**: \n\nEvaluates if the answer is faithful to the retrieved contexts (in other words, whether if there's hallucination).\n\n**Context Relevancy**: \n\nWhether retrieved context is relevant to the query. \n\n**Answer Relevancy**: \n\nWhether the generated answer is relevant to the query. \n\n**Guideline Adherence**: \n\nWhether the predicted answer adheres to specific guidelines.\n\n**Hit Rate**:\n\nHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it\u2019s about how often our system gets it right within the top few guesses.\n\n**Mean Reciprocal Rank (MRR)**:\n\nMRR serves as a metric for evaluating the accuracy of a system by examining the rank of the highest-placed relevant document for each query. It calculates the average of the reciprocals of these ranks across all queries. For instance, if the first relevant document is ranked highest, the reciprocal rank is 1; if it\u2019s second, the reciprocal rank is 1/2, and so forth.", "metadata": {"collapsed": true, "id": "e6d9ecc4-f76b-4aea-abc8-3f9afdb20ed6", "jupyter": {"outputs_hidden": true}}}, {"cell_type": "markdown", "source": "# Preparation", "metadata": {"id": "28ada75e-2110-4fbb-a53a-516a7f518a2d"}}, {"cell_type": "markdown", "source": "Run the cell below and restart the kernel. Once the kernel restarts, skip to **Set Up**", "metadata": {"id": "420d046a-4b2d-465f-b31a-11d0539cf41e"}}, {"cell_type": "code", "source": "!pip install -q -U llama-index llama-index-llms-ibm pydantic\nprint(\"Done Installing!\")", "metadata": {"id": "8b85de4b-0df5-467f-9b34-69cf5482fe8f"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Set Up", "metadata": {"id": "1a70301a-26e4-46bf-b087-3d6df171e665"}}, {"cell_type": "markdown", "source": "Before we measure the performances, we need a way to tell WatsonX.AI that it's an authroized user trying to use the models. Follow the instruction below to retrieve your API Key.\n\nInstruction:\n\nWhen done, paste **your api key below** and run both cells", "metadata": {"id": "ba97c771-bda7-4161-84f4-bcb3994ebe33"}}, {"cell_type": "code", "source": "myApikey=\"Paste your API key here\"", "metadata": {"id": "cfc1cf06-ca02-4f70-a1d0-8c38a9608cc7"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "\nfrom llama_index.core.evaluation import BatchEvalRunner\nfrom llama_index.core.evaluation import AnswerRelevancyEvaluator\nfrom llama_index.core.evaluation import CorrectnessEvaluator\nfrom llama_index.llms.ibm import WatsonxLLM\nfrom statistics import mean\nimport os\nimport nest_asyncio\nimport asyncio\nimport warnings\nimport pandas as pd\nimport re\npd.set_option('display.max_colwidth', None)\nwarnings.filterwarnings(\"ignore\")\n\nnest_asyncio.apply()\n\nwatsonx_llm = WatsonxLLM(\n    model_id=\"mistralai/mistral-large\", # granite or mistral \n    url=\"https://us-south.ml.cloud.ibm.com\",\n    apikey=myApikey,\n    project_id=os.getenv(\"PROJECT_ID\"),\n    max_new_tokens=1000\n)", "metadata": {"id": "1df43797-7cf3-4648-8d59-f5101ed5c143"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "async def evaluate_rag(log,ground_truth=None):\n\n    # The metrics to be computed is: Answer Relevancy\n    pattern = r'<|.*?\\|>'\n    parsed_list=[x.replace(\"\\n\",\"\") for x in re.split(pattern, log) if len(x)>2]\n    queries=parsed_list[1::2] \n    responses=parsed_list[2::2] \n    \n    ar_eval=AnswerRelevancyEvaluator(llm=watsonx_llm)\n\n    ar_runner = BatchEvalRunner(\n        {\"answer_relevancy\": ar_eval},workers=5,show_progress=True)\n\n    eval_results_ar = await ar_runner.aevaluate_response_strs(queries=queries, response_strs=responses)\n    ar_score=mean([float(eval_results_ar['answer_relevancy'][x].score) for x in range(len(queries))])*100\n    \n    results=[eval_results_ar['answer_relevancy'][x].feedback for x in range(len(queries))]\n    feedback, scores = map(list, zip(*(s.split(\"\\n\\n\") for s in results)))\n    feedback=[x.replace(\"\\n\",\"\") for x in feedback]\n    print(\"\\n\")\n    \n    print(f\"The average Answer Relevancy score is {round(ar_score,2)}%\")\n    print(\"\\n\")\n\n    df=pd.DataFrame(data={'Query':queries,'Response':responses,'Feedback':feedback,'Score (max:2)':scores})\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n        display(df)\n    \n\n\n\n    \n\n    ", "metadata": {"id": "d8beea0d-6147-4628-a51d-9673edacd292"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "With the LLM connection establsihed, copy paste your RAG log from your Prompt Lab session and run both cells.", "metadata": {"id": "dad293c7-e252-4cc4-9509-5d8678ba18e0"}}, {"cell_type": "markdown", "source": "The responses will be evaluated against the following two criteria: <br> 1. Does the provided response match the subject matter of the user's query? <br> 2. Does the provided response attempt to address the focus or perspective on the subject matter taken on by the user's query? <br> Each question above is worth 1 point.", "metadata": {"id": "0342a2d2-a964-4f2f-a9bc-bee7480f91ea"}}, {"cell_type": "code", "source": "myLog=\"\"\"Copy-paste your log here\n\"\"\"", "metadata": {"id": "edf9eb1e-d575-4add-a11b-ba9001c19048"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "await evaluate_rag(myLog)", "metadata": {"id": "778a4bd0-91d1-4fc0-a8c9-50710c8136d6"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "11a3b7e8-d044-4951-acf4-4c16a189e802"}, "outputs": [], "execution_count": null}]}